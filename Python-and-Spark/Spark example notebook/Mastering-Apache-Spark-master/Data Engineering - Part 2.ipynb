{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/s3data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"dbfs:/mnt/s3data/listing.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_listing = spark.read.format(file_type) \\\n",
    "  .option(\"multiline\",\"true\") \\\n",
    "  .option(\"quote\", \"\\\"\") \\\n",
    "  .option(\"escape\", \"\\\"\") \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "df_listing=df_listing.withColumn(\"scrape_id\",col(\"scrape_id\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# File location and type\n",
    "file_location = \"dbfs:/mnt/s3data/neighbourhoods.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "neighbourhoodSchema = StructType([\n",
    "    StructField(\"neighbourhood_group\", StringType(), True),        \n",
    "    StructField(\"neighbourhood\", StringType(), True)\n",
    "])\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_neighbourhoods = spark.read.format(file_type) \\\n",
    "  .option(\"infer_schema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df_neighbourhoods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"dbfs:/mnt/s3data/reviews_fixed/\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_reviews = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_listing.groupBy('property_type').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_listing.groupBy('property_type').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing.groupBy(\"property_type\",\"price\").agg(({\"price\": \"average\"})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select distinct price from listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def trim_char(string):\n",
    "    return string.strip('$')\n",
    "\n",
    "spark.udf.register(\"trim_func\", trim_char)\n",
    "\n",
    "trim_func_udf = udf(trim_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing.select(\"property_type\",\"price\",trim_func_udf(\"price\").cast(FloatType()).alias(\"price_f\")).groupBy(\"property_type\").agg(({\"price_f\":\"average\"})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing.select(\"property_type\",\"price\",trim_func_udf(\"price\").cast(FloatType()).alias(\"price_f\")).groupBy(\"property_type\").agg(({\"price_f\":\"average\"})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_listing.select(\"property_type\",\"price\",trim_func_udf(\"price\").cast(FloatType()).alias(\"price_f\")).groupBy(\"property_type\").agg(({\"price_f\":\"average\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "df_neigh_count=df_listing.groupBy(\"neighbourhood\").agg(({\"neighbourhood\":\"count\"})).withColumnRenamed(\"count(neighbourhood)\", \"count\")\n",
    "df_neigh_count = df_neigh_count.withColumn('percentage', round((f.col('count')/f.sum('count').over(Window.partitionBy())*100),3))\n",
    "display(df_neigh_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_neigh_count.orderBy('percentage', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei_group=df_neigh_count.alias(\"nc\").join(df_neighbourhoods.alias(\"ne\"),col(\"nc.neighbourhood\") == col(\"ne.neighbourhood\")).select(\"nc.*\",\"ne.neighbourhood_group\")\n",
    "\n",
    "display(nei_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nei_group.sort(desc('percentage')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "display(df_listing.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_listing.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.groupBy(\"listing_id\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing.alias(\"list\").join(df_reviews.alias(\"rev\"), col(\"list.id\")==col(\"rev.listing_id\")).select(\"list.id\").groupBy(\"list.id\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing.alias(\"list\").join(df_reviews.alias(\"rev\"), col(\"list.id\")==col(\"rev.listing_id\"), how=\"left\").select(\"list.id\", \"rev.listing_id\").filter(col(\"rev.listing_id\").isNull()).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.groupBy(\"reviewer_id\").count().filter(col(\"count\") > 30 ).orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.groupBy(\"reviewer_id\").count().filter(col(\"count\") > 30).orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_reviews.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_fil=df_reviews.filter(f.col(\"reviewer_id\").cast(\"int\").isNotNull().alias(\"Value \"))\n",
    "display(df_review_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_fil.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_fil.groupBy(\"reviewer_id\").count().filter(col(\"count\") > 30).orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_cnt=df_review_fil.groupBy(\"reviewer_id\").count().filter(col(\"count\") > 30)\n",
    "display(df_review_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dfunc =  udf (lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())\n",
    "\n",
    "spark.udf.register(\"datec\",dfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_fil.withColumn(\"datetime\", dfunc(col(\"date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_dt=df_review_fil.withColumn(\"datetime\", dfunc(col(\"date\"))).groupBy(\"reviewer_id\").agg(f.min('datetime'), f.max('datetime'))\n",
    "display(df_review_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_summary=df_review_cnt.join(df_review_dt, \"reviewer_id\").withColumn(\"date_diff\",datediff(\"max(datetime)\",\"min(datetime)\")).orderBy('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_review_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_summary.createOrReplaceTempView(\"review_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from review_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "describe review_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select reviewer_id, date_diff/count as avg_days from review_summary order by avg_days asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select distinct listing_id, neighbourhood, price, count(listing_id) over (partition by listing_id order by listing_id) as review_cnt from (\n",
    "select listing_id, neighbourhood, price, reviewer_id \n",
    "from listing a left join reviews b on a.id = b.listing_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select\n",
    "id, neighbourhood, price, review_cnt,\n",
    "dense_rank() OVER (PARTITION BY neighbourhood ORDER BY review_cnt DESC) as rank\n",
    "from\n",
    "(\n",
    "select distinct id, neighbourhood, price, count(id) over (partition by id order by id) as review_cnt from (\n",
    "select a.id, neighbourhood, price, reviewer_id \n",
    "from listing a left join reviews b on a.id = b.listing_id\n",
    ") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select\n",
    "id, neighbourhood, price, review_cnt, rank\n",
    "from (\n",
    "select\n",
    "id, neighbourhood, price, review_cnt,\n",
    "dense_rank() OVER (PARTITION BY neighbourhood ORDER BY review_cnt DESC) as rank\n",
    "from\n",
    "(\n",
    "select distinct id, neighbourhood, price, count(id) over (partition by id order by id) as review_cnt from (\n",
    "select a.id, neighbourhood, price, reviewer_id \n",
    "from listing a left join reviews b on a.id = b.listing_id\n",
    ") \n",
    ")) where rank <= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select room_type, avg(trim_func(price)) as avg_price, count(*) as count from listing group by room_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "regexp_extract(comments, '(good|excellent|amazing)', 1),\n",
    "comments\n",
    "FROM\n",
    "reviews\n",
    "where regexp_extract(comments, '(good|excellent|amazing)', 1) != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "regexp_extract(comments, '(bad|worse|wrong)', 1),\n",
    "comments\n",
    "FROM\n",
    "reviews\n",
    "where regexp_extract(comments, '(bad|worse|wrong)', 1) != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select amenities, split, exploded\n",
    "from (\n",
    "select amenities, split(amenities, \",\") as split from listing\n",
    ") LATERAL VIEW explode(split) as exploded limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select price, CAST(trim_func(price) as float), ntile(5) over(order by CAST(trim_func(price) as float)) from listing where neighbourhood='Corona' order by CAST(trim_func(price) as float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select reviewer_id, cur_date, prev_date from (\n",
    "select reviewer_id, datec(date) as cur_date, lag(datec(date)) over (partition by reviewer_id order by datec(date)) as prev_date from reviews \n",
    "where date is not null and date like '%2017%' order by datec(date)\n",
    ") where prev_date is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select reviewer_id, total_spent, CUME_DIST() OVER ( ORDER BY total_spent ) as cum_dist from (\n",
    "select reviewer_id, sum(CAST(trim_func(l.price) as float)) as total_spent from reviews as r join listing as l on r.reviewer_id = l.id group by reviewer_id order by 2 desc limit 20) where total_spent is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Data Engineering - Part 2",
  "notebookId": 2610328142917247
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
